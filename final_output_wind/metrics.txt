Epoch 1/100, Total Reward: -246730.6232984735
Epoch 2/100, Total Reward: 160060.54707832696
Epoch 3/100, Total Reward: 161806.3010783248
Epoch 4/100, Total Reward: 158569.70123043237
Epoch 5/100, Total Reward: 160298.80553298933
Epoch 6/100, Total Reward: 159821.4689087451
Epoch 7/100, Total Reward: 158998.42144099687
Epoch 8/100, Total Reward: 158871.8197821527
Epoch 9/100, Total Reward: 159986.72194588414
Epoch 10/100, Total Reward: 159550.69008362104
Epoch 11/100, Total Reward: 159471.1725739512
Epoch 12/100, Total Reward: 159781.07404263725
Epoch 13/100, Total Reward: 159544.9596873964
Epoch 14/100, Total Reward: 160299.22143155223
Epoch 15/100, Total Reward: 159530.12765488634
Epoch 16/100, Total Reward: 158957.16053409447
Epoch 17/100, Total Reward: 157996.77927830696
Epoch 18/100, Total Reward: 159840.2582325693
Epoch 19/100, Total Reward: 160706.8214256056
Epoch 20/100, Total Reward: 158189.13667307235
Epoch 21/100, Total Reward: 159935.3557539633
Epoch 22/100, Total Reward: 159077.0434976073
Epoch 23/100, Total Reward: 159874.95739426836
Epoch 24/100, Total Reward: 160158.79147298174
Epoch 25/100, Total Reward: 159335.6593285375
...
Epoch 98/100, Total Reward: 160972.73025133563
Epoch 99/100, Total Reward: 158672.35894053272
Epoch 100/100, Total Reward: 159828.57325432808
Average time per epoch :  0.34382746458053587


Final Policy Parameters (Actor Network):
0.weight: tensor([[-9.3230e-02, -1.7168e-01,  3.1691e-01],
        [-4.8902e-01, -2.4889e-01, -1.4500e-01],
        [ 2.8960e-01,  5.0076e-01, -6.4668e-02],
        [ 1.7976e-01, -3.9614e-01,  5.4634e-01],
        [ 1.4051e-01, -3.9512e-01,  4.5540e-01],
        [-3.3937e-01,  4.1692e-01, -5.6097e-01],
        [-1.2530e-01, -2.9806e-01,  1.8486e-03],
        [ 2.6408e-01,  5.0809e-01,  4.7115e-02],
        [ 3.7508e-01,  2.8211e-01,  5.5169e-01],
        [ 5.0845e-01,  5.3078e-01, -1.8874e-01],
        [-3.4019e-01,  2.8718e-01, -2.1238e-01],
        [ 3.7584e-02,  2.5370e-01, -7.9641e-02],
        [ 2.7704e-01,  2.8058e-01, -4.7494e-01],
        [ 4.3926e-01, -4.1000e-01,  2.1433e-01],
        [ 6.9104e-02, -4.4683e-01,  5.0355e-01],
        [ 5.4866e-01, -2.2177e-01, -4.8524e-01],
        [ 4.5412e-01,  1.8098e-01,  5.5776e-01],
        [-4.0483e-01,  4.8246e-01,  2.8206e-01],
        [ 1.2418e-01,  5.9727e-02,  1.6999e-01],
        [ 4.5121e-01, -1.7580e-01, -1.7771e-01],
        [ 4.6836e-02,  5.6942e-01, -4.5729e-01],
        [ 8.7486e-02, -1.4570e-01, -4.0710e-01],
        [-2.9132e-01,  1.4533e-01,  2.4155e-01],
...
          3.2376e-02,  5.5808e-02,  5.8532e-02, -3.5359e-02, -2.5931e-03,
         -5.1600e-02, -4.6022e-02,  4.0416e-02,  6.0220e-02,  2.2687e-02,
         -4.5908e-02]])
4.bias: tensor([ 0.0092, -0.0237])